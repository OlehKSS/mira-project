{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-atlas Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir, join\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy.linalg import inv, det, norm\n",
    "from math import sqrt, pi\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./src/nib-tools.py\"\n",
    "%run \"./src/file-tools.py\"\n",
    "%run \"./src/em.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Weights from Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IBSR_18', 'IBSR_03', 'IBSR_01', 'IBSR_06', 'IBSR_09', 'IBSR_07', 'IBSR_16', 'IBSR_08', 'IBSR_05', 'IBSR_04']\n",
      "['IBSR_13', 'IBSR_12', 'IBSR_17', 'IBSR_11', 'IBSR_14']\n"
     ]
    }
   ],
   "source": [
    "# Path to test image\n",
    "# possible values \n",
    "data_reg_params = [\"Par0000-no-matching\", \"Par0009-no-matching\", \"Par0009-matched-to-mni\"]\n",
    "# regular majority voting or weighted voting multi-atlas\n",
    "modes = ['ma', 'weighted_ma']\n",
    "data_reg_param = data_reg_params[0]\n",
    "mode = modes[1]\n",
    "\n",
    "valid_img_path = f\"./registered-data/{data_reg_param}/Validation_Set/templates/\"\n",
    "gt_path = f\"./registered-data/{data_reg_param}/Validation_Set/labels/\"\n",
    "train_labels_path = f\"./registered-data/{data_reg_param}/Training_Set/labels/\"\n",
    "train_img_path = f\"./registered-data/{data_reg_param}/Training_Set/templates/\"\n",
    "\n",
    "train_dirs = [f for f in listdir(train_img_path) if isdir(join(train_img_path, f))]\n",
    "valid_dirs = [f for f in listdir(valid_img_path) if isdir(join(valid_img_path, f))]\n",
    "\n",
    "# read training labels\n",
    "train_labels = []\n",
    "for f in train_dirs:\n",
    "    labels, _ = read_im(join(train_labels_path, f, 'result.nii.gz'))\n",
    "    train_labels.append(labels)\n",
    "\n",
    "print(train_dirs)\n",
    "print(valid_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_metrics(train_dir_path):\n",
    "    \"\"\"\n",
    "    Get similarity metrics from elastix log files.\n",
    "    \n",
    "    Args:\n",
    "        train_dir_path (str): the path to a folder with registration results.\n",
    "    \n",
    "    Returns:\n",
    "        (numpy.ndarray): similarity metrics.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    dirs = tuple(f for f in listdir(train_dir_path) if isdir(join(train_dir_path, f)))\n",
    "    sim_metrics = np.zeros(len(dirs))\n",
    "    \n",
    "    for index, train_dir in enumerate(dirs):\n",
    "        elastix_log_file = open(join(train_dir_path, train_dir, 'elastix.log'), \"r\")\n",
    "        elastix_log = elastix_log_file.read()\n",
    "        elastix_log_file.close()\n",
    "        reg_expr = r'^Final metric value\\s+=\\s+([\\d\\.\\-]+)$'\n",
    "        \n",
    "        matches = re.findall(reg_expr, elastix_log, re.M)\n",
    "        \n",
    "        # 0th value - MI after affine transformation, 1st value - MI after B-splines transformation\n",
    "        sim_metrics[index] = matches[1]\n",
    "        \n",
    "    return sim_metrics\n",
    "\n",
    "\n",
    "def majority_vote_masks(train_labels, weights=None):\n",
    "    \"\"\"\n",
    "    Get segmentation masks for CSF, GM, WM tissues.\n",
    "    \n",
    "    Args:\n",
    "        train_labels ([numpy.ndarray]): an iterable of training labels (ground-truth segmentations).\n",
    "        weights ([float]): an iterable with weights for each label, should have the same length\n",
    "            as train_labels, optional.\n",
    "    \n",
    "    Returns:\n",
    "        csf_mask, gm_mask, wm_mask (tuple(numpy.ndarray)): a tuple of masks for tissue segmentation.\n",
    "    \"\"\"\n",
    "    csf_mask = None\n",
    "    gm_mask = None\n",
    "    wm_mask = None\n",
    "    \n",
    "    if weights is None:\n",
    "        weights = np.ones(len(train_labels))\n",
    "\n",
    "    for labels, w in zip(train_labels, weights):\n",
    "        csf_vote = np.zeros_like(labels, dtype=float)\n",
    "        gm_vote = np.zeros_like(labels, dtype=float)\n",
    "        wm_vote = np.zeros_like(labels, dtype=float)\n",
    "\n",
    "        csf_vote[labels == CSF_label] = w\n",
    "        gm_vote[labels == GM_label] = w\n",
    "        wm_vote[labels == WM_label] = w\n",
    "\n",
    "        if (csf_mask is None) and (gm_mask is None) and (wm_mask is None):\n",
    "            csf_mask = csf_vote\n",
    "            gm_mask = gm_vote\n",
    "            wm_mask = wm_vote\n",
    "        else:\n",
    "            csf_mask += csf_vote\n",
    "            gm_mask += gm_vote\n",
    "            wm_mask += wm_vote\n",
    "    \n",
    "    return csf_mask, gm_mask, wm_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'ma':\n",
    "    maj_vote_csf_mask, maj_vote_gm_mask, maj_vote_wm_mask = majority_vote_masks(train_labels)\n",
    "elif mode == 'weighted_ma':\n",
    "    sim_metrics = get_sim_metrics(train_img_path)\n",
    "    sim_metrics = -1 * sim_metrics\n",
    "    maj_vote_csf_mask, maj_vote_gm_mask, maj_vote_wm_mask = majority_vote_masks(train_labels, weights=sim_metrics)\n",
    "\n",
    "CSF_atlas = normalize(maj_vote_csf_mask, max_val=1.0)\n",
    "GM_atlas = normalize(maj_vote_gm_mask, max_val=1.0)\n",
    "WM_atlas = normalize(maj_vote_wm_mask, max_val=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atlas and Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to test image\n",
    "test_img_path = valid_img_path\n",
    "\n",
    "# Path to atlas image folder\n",
    "result_path = f\"./segmentation-results/{data_reg_param}/hist-segmentation-{mode}/\"\n",
    "out_dice_path = f'./dice-results/{data_reg_param}_hist_dice-{mode}.csv'\n",
    "os.mkdir(result_path)\n",
    "\n",
    "    \n",
    "def get_hist_probs(img, hist):\n",
    "    \"\"\"Returns probability values for an image based on a given histogram.\n",
    "    \n",
    "    Parameters:\n",
    "        img (np.ndarray): input image.\n",
    "        hist (np.ndarray): histogram.\n",
    "    \n",
    "    Returns:\n",
    "        out_prob (np.ndarray): propability image.\n",
    "    \"\"\"\n",
    "    out_prob = np.zeros_like(img, dtype='float')\n",
    "    # we assume that intensity values and indices of bins correspond\n",
    "    for intens_val in np.nditer(np.nonzero(hist)):\n",
    "        out_prob[img == intens_val] = hist[intens_val]\n",
    "    \n",
    "    return out_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_names = [f for f in listdir(train_im_path) if isdir(join(train_im_path, f))]\n",
    "\n",
    "# Initialize arrays to hold intensity values \n",
    "# CSF_intensities, WM_intensities, GM_intensities\n",
    "for i, f in enumerate(im_names):\n",
    "    # Load all data for histogram\n",
    "    test_data, test_img = read_im(join(train_im_path,f,'result.1.nii.gz'))\n",
    "    test_data = normalize(test_data, max_intens)\n",
    "    labels_data, labels_img = read_im(join(train_labels_path,f,'result.nii.gz'))\n",
    "\n",
    "    if i == 0:\n",
    "        # Initialize arrays to hold intensity values \n",
    "        CSF_intensities = test_data[labels_data == CSF_label]\n",
    "        WM_intensities = test_data[labels_data == WM_label]\n",
    "        GM_intensities = test_data[labels_data == GM_label]\n",
    "    else:    \n",
    "        CSF_intensities = np.append(CSF_intensities, test_data[labels_data == CSF_label])\n",
    "        WM_intensities = np.append(WM_intensities, test_data[labels_data == WM_label])\n",
    "        GM_intensities = np.append(GM_intensities, test_data[labels_data == GM_label])\n",
    "\n",
    "# get histograms\n",
    "bins = tuple(range(0, max_intens+1))\n",
    "CSF_hist, _ = np.histogram(CSF_intensities, bins, density=True)\n",
    "GM_hist, _ = np.histogram(GM_intensities, bins, density=True)\n",
    "WM_hist, _ = np.histogram(WM_intensities, bins, density=True)\n",
    "bins = bins[:-1]\n",
    "\n",
    "# plot histograms\n",
    "plt.plot(bins, CSF_hist)\n",
    "plt.title(\"CSF Histogram\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(bins, GM_hist)\n",
    "plt.title(\"GM Histogram\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(bins, WM_hist)\n",
    "plt.title(\"WM Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlydirs = [f for f in listdir(test_im_path) if isdir(join(test_im_path, f))]\n",
    "\n",
    "all_dice = np.zeros((len(onlydirs),3))\n",
    "\n",
    "GM_atlas, _ = read_im(atlas_path+'GM_probs.nii.gz')\n",
    "WM_atlas, _ = read_im(atlas_path+'WM_probs.nii.gz')\n",
    "CSF_atlas, _ = read_im(atlas_path+'CSF_probs.nii.gz')\n",
    "\n",
    "for i, f in enumerate(onlydirs):\n",
    "    test_data, template_img = read_im(join(test_im_path,f,'result.1.nii.gz'))\n",
    "    test_data = normalize(test_data, max_intens)\n",
    "\n",
    "    # Incorporate historgram from above\n",
    "    # compute probabilities based on intensity histograms\n",
    "    GM_hist_prob = get_hist_probs(test_data, GM_hist)\n",
    "    WM_hist_prob = get_hist_probs(test_data, WM_hist)\n",
    "    CSF_hist_prob = get_hist_probs(test_data, CSF_hist)\n",
    "\n",
    "    # combine those probabilities\n",
    "    GM_hist_prob *= GM_atlas\n",
    "    WM_hist_prob *= WM_atlas\n",
    "    CSF_hist_prob *= CSF_atlas\n",
    "\n",
    "    # Assign GM, WM, CSF to voxel with highest probability\n",
    "    GM = GM_label * np.nan_to_num((GM_hist_prob > CSF_hist_prob) * (GM_hist_prob > WM_hist_prob))\n",
    "    WM = WM_label * np.nan_to_num((WM_hist_prob > CSF_hist_prob) * (WM_hist_prob > GM_hist_prob))\n",
    "    CSF = CSF_label * np.nan_to_num((CSF_hist_prob > WM_hist_prob) * (CSF_hist_prob > GM_hist_prob))\n",
    "    seg_im = GM + WM + CSF\n",
    "    segmented_img = nib.Nifti1Image(seg_im, template_img.affine, template_img.header)\n",
    "\n",
    "    # Calculate DICE\n",
    "    path_gt = join(gt_path,f,\"result.nii.gz\")\n",
    "    _, groundtruth_img = read_im(path_gt)\n",
    "    all_dice[i,0], all_dice[i,1], all_dice[i,2] = dice_similarity(segmented_img, groundtruth_img)\n",
    "\n",
    "    # Make directory to save result seg\n",
    "    new_dir = join(result_path,f)\n",
    "    os.mkdir(new_dir)\n",
    "    nib.save(segmented_img, join(new_dir,'atlas_hist_seg.nii.gz'))\n",
    "\n",
    "print(all_dice)\n",
    "with open(out_dice_path, 'w+') as out_f:\n",
    "    out_f.write('img,csf,gm,wm,\\n')\n",
    "    for index, row in enumerate(all_dice): \n",
    "        out_f.write(onlydirs[index] + ',' + ','.join(str(j) for j in row) + ',\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM+Multi-atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_intens = 255\n",
    "\n",
    "MAX_STEPS = 30\n",
    "min_change = 0.01\n",
    "\n",
    "test_img_path = valid_img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************\n",
      "Img IBSR_13\n",
      "Step 0\n",
      "Distribution change 0.007467\n",
      "0.010219753129274386 0.354797251268109 0.6349829956047053\n",
      "Loop stopped\n",
      "********************************************************************\n",
      "Img IBSR_12\n",
      "Step 0\n",
      "Distribution change 0.002434\n",
      "0.014693748923909868 0.3519376141885034 0.6333686368889736\n",
      "Loop stopped\n",
      "********************************************************************\n",
      "Img IBSR_17\n",
      "Step 0\n",
      "Distribution change 0.011518\n",
      "0.008921065738222786 0.35730243114370314 0.6337765031183079\n",
      "********************************************************************\n",
      "Img IBSR_17\n",
      "Step 1\n",
      "Distribution change 0.001789\n",
      "0.007657277498857278 0.3850509390802253 0.6072917834207219\n",
      "Loop stopped\n",
      "********************************************************************\n",
      "Img IBSR_11\n",
      "Step 0\n",
      "Distribution change 0.007399\n",
      "0.01422544178828696 0.36281619350998595 0.6229583647019328\n",
      "Loop stopped\n",
      "********************************************************************\n",
      "Img IBSR_14\n",
      "Step 0\n",
      "Distribution change 0.005401\n",
      "0.010401542004141518 0.3516413757604795 0.637957082236307\n",
      "Loop stopped\n"
     ]
    }
   ],
   "source": [
    "# Path to atlas image folder\n",
    "result_path = f\"./segmentation-results/{data_reg_param}/em-segmentation-{mode}/\"\n",
    "out_dice_path = f'./dice-results/{data_reg_param}_em_dice-{mode}.csv'\n",
    "os.mkdir(result_path)\n",
    "\n",
    "onlydirs = [f for f in listdir(test_img_path) if isdir(join(test_img_path, f))]\n",
    "\n",
    "all_dice = np.zeros((len(onlydirs),3))\n",
    "\n",
    "GM_atlas_flat = GM_atlas.flatten()\n",
    "WM_atlas_flat = WM_atlas.flatten()\n",
    "CSF_atlas_flat = CSF_atlas.flatten()\n",
    "mask = GM_atlas + WM_atlas + CSF_atlas\n",
    "mask_data = mask.copy().flatten()\n",
    "mask_data = np.transpose(mask_data)\n",
    "\n",
    "for i, f in enumerate(onlydirs):\n",
    "    # Load all data for EM algorithm\n",
    "    test_data, test_img = read_im(join(test_img_path, f, 'result.1.nii.gz'))\n",
    "    test_data = normalize(test_data, max_intens)\n",
    "\n",
    "    _, groundtruth_img = read_im(join(gt_path, f, \"result.nii.gz\"))\n",
    "\n",
    "    # Pre-process feature vector to remove background points from algorithm\n",
    "    # and save those indicies to add back\n",
    "    features = test_data.copy().flatten()\n",
    "    features = np.transpose(features)   \n",
    "\n",
    "    features_nonzero_row_indicies = np.nonzero(mask_data)\n",
    "    features_nonzero = features[features_nonzero_row_indicies]\n",
    "\n",
    "    # row index shifted by +1 will correspond to tissue labels from ground-truth\n",
    "    features_nonzero_pred = np.array((CSF_atlas_flat[features_nonzero_row_indicies],\n",
    "                                      WM_atlas_flat[features_nonzero_row_indicies],\n",
    "                                      GM_atlas_flat[features_nonzero_row_indicies]))\n",
    "    y_pred = np.argmax(features_nonzero_pred, axis=0)\n",
    "\n",
    "    # intialize EM algorithm\n",
    "    class0 = features_nonzero[np.argwhere(y_pred == 0)[:,0]]\n",
    "    class1 = features_nonzero[np.argwhere(y_pred == 1)[:,0]]\n",
    "    class2 = features_nonzero[np.argwhere(y_pred == 2)[:,0]]\n",
    "\n",
    "    # Compute mean and variance of each class\n",
    "    mean0 = np.mean(class0, axis = 0)\n",
    "    mean1 = np.mean(class1, axis = 0)\n",
    "    mean2 = np.mean(class2, axis = 0)\n",
    "    cov0 = np.cov(class0, rowvar = False)\n",
    "    cov1 = np.cov(class1, rowvar = False)\n",
    "    cov2 = np.cov(class2, rowvar = False)\n",
    "\n",
    "    # Class distribution\n",
    "    a0 = class0.shape[0] / features_nonzero.shape[0]\n",
    "    a1 = class1.shape[0] / features_nonzero.shape[0]\n",
    "    a2 = class2.shape[0] / features_nonzero.shape[0]\n",
    "\n",
    "    # Compute Gaussian mixture model for each point\n",
    "    p0 = gaussian_mixture(features_nonzero,  mean = mean0, cov = cov0)\n",
    "    p1 = gaussian_mixture(features_nonzero,  mean = mean1, cov = cov1)\n",
    "    p2 = gaussian_mixture(features_nonzero,  mean = mean2, cov = cov2)\n",
    "\n",
    "    # # Compute membership weight for each point\n",
    "    weights = membership_weight(p0, p1, p2, a0, a1, a2)\n",
    "    # get initial log-likelihood\n",
    "    log_likelihood = get_log_likelihood((a0, a1, a2), (p0, p1, p2))\n",
    "\n",
    "    n_steps = 0\n",
    "\n",
    "    while True:\n",
    "        # Maximization step: Use that classification to reestimate the parameters\n",
    "        # Class distribution\n",
    "        counts = np.sum(weights, axis=0)\n",
    "\n",
    "        a0 = counts[0] / len(features_nonzero)\n",
    "        a1 = counts[1] / len(features_nonzero)\n",
    "        a2 = counts[2] / len(features_nonzero)\n",
    "\n",
    "        # Calculate mean and covariance for new classes\n",
    "        mean0 = (1/counts[0]) * (weights[:, 0] @ features_nonzero)\n",
    "        mean1 = (1/counts[1]) * (weights[:, 1] @ features_nonzero)\n",
    "        mean2 = (1/counts[2]) * (weights[:, 2] @ features_nonzero)\n",
    "        cov0 = (1/counts[0]) * ((weights[:, 0] * (features_nonzero - mean0)) @ (features_nonzero - mean0))\n",
    "        cov1 = (1/counts[1]) * ((weights[:, 1] * (features_nonzero - mean1)) @ (features_nonzero - mean1))\n",
    "        cov2 = (1/counts[2]) * ((weights[:, 2] * (features_nonzero - mean2)) @ (features_nonzero - mean2))\n",
    "\n",
    "        p0 = gaussian_mixture(features_nonzero,  mean = mean0, cov = cov0)\n",
    "        p1 = gaussian_mixture(features_nonzero,  mean = mean1, cov = cov1)\n",
    "        p2 = gaussian_mixture(features_nonzero,  mean = mean2, cov = cov2)\n",
    "\n",
    "        # Compute membership weight for each point\n",
    "        weights = membership_weight(p0, p1, p2, a0, a1, a2)\n",
    "\n",
    "        log_likelihood_new = get_log_likelihood((a0, a1, a2), (p0, p1, p2))\n",
    "\n",
    "        dist_change = abs((log_likelihood_new - log_likelihood) / log_likelihood)\n",
    "        print(\"********************************************************************\")\n",
    "        print(f\"Img {f}\")\n",
    "        print(\"Step %d\" % n_steps)\n",
    "        print(\"Distribution change %f\" % dist_change)\n",
    "        print(a0, a1, a2)\n",
    "\n",
    "        n_steps += 1\n",
    "\n",
    "        # check whether we reached desired precision or max number of steps\n",
    "        if (n_steps >= MAX_STEPS) or (dist_change <= min_change):\n",
    "            print(\"Loop stopped\")\n",
    "            break\n",
    "        else:\n",
    "            log_likelihood = log_likelihood_new\n",
    "\n",
    "    y_pred = np.argmax(weights, axis=1)\n",
    "    segment_nii_atlas = integrate_atlas_nii(test_img, y_pred, features_nonzero, \n",
    "                               features_nonzero_row_indicies, weights, CSF_atlas, \n",
    "                               GM_atlas, WM_atlas)\n",
    "\n",
    "    # Calculate DICE\n",
    "    all_dice[i,0], all_dice[i,1], all_dice[i,2] = dice_similarity(segment_nii_atlas, groundtruth_img)\n",
    "\n",
    "    # Make directory to save result seg\n",
    "    new_dir = join(result_path,f)\n",
    "    os.mkdir(new_dir)\n",
    "    nib.save(segment_nii_atlas, join(new_dir,'atlas_EM_seg.nii.gz'))\n",
    "\n",
    "save_dice(out_dice_path, onlydirs, all_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
